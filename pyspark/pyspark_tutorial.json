{"paragraphs":[{"text":"%md\n\n##Tutorial using Apache Spark with the PySpark API\n###Introduction\nTo run a paragraph in a Zeppelin notebook you can either click the `play` button (blue triangle) on the right-hand side or simply press `Shift + Enter`.\n\nThis tutorial is in Spark Python (PySpark). PySpark must be invoked in each cell that we wish to run Python code in by adding `%pyspark` to the beginning of a cell. Cells with text like this one are in Markdown, which is invoked by `%md`. If an interpreter is not called in a cell, then it uses the default Spark (Scala).\n\n**Reminder:** In Zeppelin, SparkContext, SQLContext, and ZeppelinContext are automically created and exposed as variable names 'sc', 'sqlContext' and 'z' in both scala and python environments. Outside of Zeppelin, these would have to be created by you. ","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 5:59:43 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476191234480_-186936932","id":"20161011-130714_1116291467","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Tutorial using Apache Spark with the PySpark API</h2>\n<h3>Introduction</h3>\n<p>To run a paragraph in a Zeppelin notebook you can either click the <code>play</code> button (blue triangle) on the right-hand side or simply press <code>Shift + Enter</code>.</p>\n<p>This tutorial is in Spark Python (PySpark). PySpark must be invoked in each cell that we wish to run Python code in by adding <code>%pyspark</code> to the beginning of a cell. Cells with text like this one are in Markdown, which is invoked by <code>%md</code>. If an interpreter is not called in a cell, then it uses the default Spark (Scala).</p>\n<p><strong>Reminder:</strong> In Zeppelin, SparkContext, SQLContext, and ZeppelinContext are automically created and exposed as variable names 'sc', 'sqlContext' and 'z' in both scala and python environments. Outside of Zeppelin, these would have to be created by you.</p>\n"},"dateCreated":"Oct 11, 2016 1:07:14 PM","dateStarted":"Oct 11, 2016 5:59:39 PM","dateFinished":"Oct 11, 2016 5:59:39 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6773"},{"text":"%md\n\n###Transformations and Actions\nAll operations are either transformations or actions\n**Transformation** operations are applied to a RDD/DataFrame to create a new RDD/DataFrame.\n**Action** operations initiate a computation and returns the result back to the driver program\nTransformations use lazy evaluation meaning the code will not execute until there is an action. This can make debugging difficult because you could have an issue in your code and not get an error when running until an action is used.\n\nTransformations covered in this tutorial:\n- filter()\n- orderBy()\n- distinct()\n- dropDuplicates()\n- select()\n- drop()\n- withColumn()\n- withColumnRenamed()\n- groupBy()\n\nActions covered in this tutorial:\n- show()\n- collect()\n- count()\n\n\n**Note:** For reference, the details of these methods along with many others not covered in this tutorial can be found in the Apache Spark's PySpark [documentation](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module). This link takes you to the latest version of Spark, but there are notes for each method that show what version it was released in or if it was changed in a particular version.","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:59:01 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476208750242_1566434454","id":"20161011-175910_1542261341","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Transformations and Actions</h3>\n<p>All operations are either transformations or actions\n<br  /><strong>Transformation</strong> operations are applied to a RDD/DataFrame to create a new RDD/DataFrame.\n<br  /><strong>Action</strong> operations initiate a computation and returns the result back to the driver program\n<br  />Transformations use lazy evaluation meaning the code will not execute until there is an action. This can make debugging difficult because you could have an issue in your code and not get an error when running until an action is used.</p>\n<p>Transformations covered in this tutorial:</p>\n<ul>\n<li>filter()</li>\n<li>orderBy()</li>\n<li>distinct()</li>\n<li>dropDuplicates()</li>\n<li>select()</li>\n<li>drop()</li>\n<li>withColumn()</li>\n<li>withColumnRenamed()</li>\n<li>groupBy()</li>\n</ul>\n<p>Actions covered in this tutorial:</p>\n<ul>\n<li>show()</li>\n<li>collect()</li>\n<li>count()</li>\n</ul>\n<p><strong>Note:</strong> For reference, the details of these methods along with many others not covered in this tutorial can be found in the Apache Spark's PySpark <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module\">documentation</a>. This link takes you to the latest version of Spark, but there are notes for each method that show what version it was released in or if it was changed in a particular version.</p>\n"},"dateCreated":"Oct 11, 2016 5:59:10 PM","dateStarted":"Oct 12, 2016 6:59:01 PM","dateFinished":"Oct 12, 2016 6:59:01 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6774"},{"text":"%md\n\nWe can confirm that the SparkContext and SQLContext have been created","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 1:22:46 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476192128407_-2023885833","id":"20161011-132208_432112010","result":{"code":"SUCCESS","type":"HTML","msg":"<p>We can confirm that the SparkContext and SQLContext have been created</p>\n"},"dateCreated":"Oct 11, 2016 1:22:08 PM","dateStarted":"Oct 11, 2016 1:22:44 PM","dateFinished":"Oct 11, 2016 1:22:44 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6775"},{"text":"sc\nsqlContext","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 7:59:46 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476192066982_-55369253","id":"20161011-132106_787044732","result":{"code":"SUCCESS","type":"TEXT","msg":"res18: org.apache.spark.SparkContext = org.apache.spark.SparkContext@6578a527\nres19: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@304e37fc\n"},"dateCreated":"Oct 11, 2016 1:21:06 PM","dateStarted":"Oct 11, 2016 7:59:47 PM","dateFinished":"Oct 11, 2016 7:59:47 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6776"},{"text":"%md\nTo check the current version of Spark","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 1:26:27 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476192300577_-2075564779","id":"20161011-132500_1913324736","result":{"code":"SUCCESS","type":"HTML","msg":"<p>To check the current version of Spark</p>\n"},"dateCreated":"Oct 11, 2016 1:25:00 PM","dateStarted":"Oct 11, 2016 1:26:26 PM","dateFinished":"Oct 11, 2016 1:26:26 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6777"},{"text":"sc.version","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 2:27:09 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476192321602_35799267","id":"20161011-132521_1338938478","result":{"code":"SUCCESS","type":"TEXT","msg":"res13: String = 1.6.1\n"},"dateCreated":"Oct 11, 2016 1:25:21 PM","dateStarted":"Oct 11, 2016 2:27:09 PM","dateFinished":"Oct 11, 2016 2:27:09 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6778"},{"text":"%md\n\n###Library Import\n\nPython libraries (packages) can be imported in same way as in Python using an `import` statement. Anaconda is also configured for this environment so all of these packages are available as well. \n*Note for if you're working in a different environment: If using Anaconda packages, it is important to ensure it has been installed and configured on every node in the cluster.*","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 3:56:00 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476191135988_-34145646","id":"20161011-130535_552350998","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Library Import</h3>\n<p>Python libraries (packages) can be imported in same way as in Python using an <code>import</code> statement. Anaconda is also configured for this environment so all of these packages are available as well.\n<br  /><em>Note for if you're working in a different environment: If using Anaconda packages, it is important to ensure it has been installed and configured on every node in the cluster.</em></p>\n"},"dateCreated":"Oct 11, 2016 1:05:35 PM","dateStarted":"Oct 11, 2016 3:55:51 PM","dateFinished":"Oct 11, 2016 3:55:51 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6779"},{"text":"%pyspark\n\n# Import the datetime library\nimport datetime\nprint 'This was last run on: {0}'.format(datetime.datetime.now())","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 2:27:47 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476196051216_1049005862","id":"20161011-142731_307128174","result":{"code":"SUCCESS","type":"TEXT","msg":"This was last run on: 2016-10-11 14:27:47.480236\n"},"dateCreated":"Oct 11, 2016 2:27:31 PM","dateStarted":"Oct 11, 2016 2:27:47 PM","dateFinished":"Oct 11, 2016 2:27:47 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6780"},{"text":"%pyspark\n\n# There are several things that may need to be imported in PySpark. These will be specific to your data analysis. Best practice is to keep all these statements at the top of your code. \n# This is only for reference...run for this cell has been disabled.\n\n# Here are just a few examples:\nfrom pyspark.sql.functions import udf, abs, toRadians, lag, lead, first, lit, round\nfrom pyspark.sql.types.import StringType\nfrom pyspark.sql import Row, functions as F\nfrom pyspark.sql.window import Window","dateUpdated":"Oct 11, 2016 2:48:45 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":false,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476196107105_-1677432113","id":"20161011-142827_2049701867","dateCreated":"Oct 11, 2016 2:28:27 PM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:6781"},{"text":"%md\n\n### Creating and working with DataFrames \n\nFirst, we will create a basic DataFrame with an array of data","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 4:04:28 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476197330874_-1183053847","id":"20161011-144850_1480270750","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Creating and working with DataFrames</h3>\n<p>First, we will create a basic DataFrame with an array of data</p>\n"},"dateCreated":"Oct 11, 2016 2:48:50 PM","dateStarted":"Oct 11, 2016 4:04:26 PM","dateFinished":"Oct 11, 2016 4:04:26 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6782"},{"text":"%pyspark\n\ndf = sqlContext.createDataFrame([(\"Roberto\", 58, \"Professor\", \"Nevada\", 714), \n                                 (\"Zoe\", 25, \"Lawyer\", \"California\", 811), \n                                 (\"Robin\", 30, \"Reporter\", \"California\", 639), \n                                 (\"Edward\", 72, \"Retired\", \"Florida\", 790), \n                                 (\"Jack\", 16, \"Student\", \"California\", None), \n                                 (\"Zoe\", 25, \"Lawyer\", \"California\", 811), \n                                 (\"Jack\", 23, \"Bartender\", \"Nevada\", 564)], \n                                 (\"name\", \"age\", \"occupation\", \"state\", \"credit_score\"))","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 12:53:33 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476197796261_1244384972","id":"20161011-145636_1158780661","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Oct 11, 2016 2:56:36 PM","dateStarted":"Oct 13, 2016 12:53:33 PM","dateFinished":"Oct 13, 2016 12:53:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6783"},{"text":"%md\n\nWe can check the type `sql.Context.createDataFrame()` to see what it returned ","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 5:51:50 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":false,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476201185545_1216724613","id":"20161011-155305_729038593","result":{"code":"SUCCESS","type":"HTML","msg":"<p>We can check the type <code>sql.Context.createDataFrame()</code> to see what it returned</p>\n"},"dateCreated":"Oct 11, 2016 3:53:05 PM","dateStarted":"Oct 11, 2016 5:51:50 PM","dateFinished":"Oct 11, 2016 5:51:50 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6784"},{"text":"%pyspark\n\nprint 'type of df: {0}'.format(type(df))","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 5:50:45 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476201199080_-740493041","id":"20161011-155319_401916922","result":{"code":"SUCCESS","type":"TEXT","msg":"type of df: <class 'pyspark.sql.dataframe.DataFrame'>\n"},"dateCreated":"Oct 11, 2016 3:53:19 PM","dateStarted":"Oct 12, 2016 5:50:45 PM","dateFinished":"Oct 12, 2016 5:50:45 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6785"},{"text":"%md\n\nDataFrames require a schema, which is a list of columns. The column types will be inferred from the data. If schema is None it will try to infer the schema (column names and types) from the data.\n\n`createDataFrame(data, schema)`","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 4:06:13 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476201501163_-2110738761","id":"20161011-155821_240942615","result":{"code":"SUCCESS","type":"HTML","msg":"<p>DataFrames require a schema, which is a list of columns. The column types will be inferred from the data. If schema is None it will try to infer the schema (column names and types) from the data.</p>\n<p><code>createDataFrame(data, schema)</code></p>\n"},"dateCreated":"Oct 11, 2016 3:58:21 PM","dateStarted":"Oct 11, 2016 4:06:05 PM","dateFinished":"Oct 11, 2016 4:06:05 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6786"},{"text":"%pyspark\n\ndf.printSchema()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 5:50:49 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476202021030_680580572","id":"20161011-160701_474094229","result":{"code":"SUCCESS","type":"TEXT","msg":"root\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- occupation: string (nullable = true)\n |-- state: string (nullable = true)\n |-- credit_score: long (nullable = true)\n\n"},"dateCreated":"Oct 11, 2016 4:07:01 PM","dateStarted":"Oct 12, 2016 5:50:49 PM","dateFinished":"Oct 12, 2016 5:50:49 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6787"},{"text":"%md\n\n**dtypes** is another way to look at column names and their data type. This returns a list.","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 6:00:37 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476202066107_-613668756","id":"20161011-160746_921026975","result":{"code":"SUCCESS","type":"HTML","msg":"<p><strong>dtypes</strong> is another way to look at column names and their data type. This returns a list.</p>\n"},"dateCreated":"Oct 11, 2016 4:07:46 PM","dateStarted":"Oct 11, 2016 6:00:33 PM","dateFinished":"Oct 11, 2016 6:00:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6788"},{"text":"%pyspark\n\nprint df.dtypes","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 5:50:56 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476202193378_1787903754","id":"20161011-160953_427804429","result":{"code":"SUCCESS","type":"TEXT","msg":"[('name', 'string'), ('age', 'bigint'), ('occupation', 'string'), ('state', 'string'), ('credit_score', 'bigint')]\n"},"dateCreated":"Oct 11, 2016 4:09:53 PM","dateStarted":"Oct 12, 2016 5:50:56 PM","dateFinished":"Oct 12, 2016 5:50:56 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6789"},{"text":"%md\n###Use **show** to view the results\n`show()` prints the first n rows. Default: n=20, truncate=True. These parameters can be changed, example: \n    df.show(2, truncate=False)","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 4:51:33 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476197807626_-1846302943","id":"20161011-145647_301293938","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Use <strong>show</strong> to view the results</h3>\n<p><code>show()</code> prints the first n rows. Default: n=20, truncate=True. These parameters can be changed, example:</p>\n<pre><code>df.show(2, truncate=False)\n</code></pre>\n"},"dateCreated":"Oct 11, 2016 2:56:47 PM","dateStarted":"Oct 11, 2016 6:01:21 PM","dateFinished":"Oct 11, 2016 6:01:21 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6790"},{"text":"%pyspark\n\ndf.show()","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 12:53:40 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476197850846_-718341761","id":"20161011-145730_1757857584","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+---+----------+----------+------------+\n|   name|age|occupation|     state|credit_score|\n+-------+---+----------+----------+------------+\n|Roberto| 58| Professor|    Nevada|         714|\n|    Zoe| 25|    Lawyer|California|         811|\n|  Robin| 30|  Reporter|California|         639|\n| Edward| 72|   Retired|   Florida|         790|\n|   Jack| 16|   Student|California|        null|\n|    Zoe| 25|    Lawyer|California|         811|\n|   Jack| 23| Bartender|    Nevada|         564|\n+-------+---+----------+----------+------------+\n\n"},"dateCreated":"Oct 11, 2016 2:57:30 PM","dateStarted":"Oct 13, 2016 12:53:40 PM","dateFinished":"Oct 13, 2016 12:53:41 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6791"},{"text":"%md\n### **Collect** \n`collect()` is another way to display the DataFrame by gathering the entries from all partitions into the driver. \nWARNING: The data returned to the driver must fit into the driver's available memory. If not, the driver will crash. This is only to be used if returned a small amount of data to the driver. `show()` is a better way to visualize the data and not worry about memory.","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 6:04:24 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476197874981_-1329677115","id":"20161011-145754_316151869","result":{"code":"SUCCESS","type":"HTML","msg":"<h3><strong>Collect</strong></h3>\n<p><code>collect()</code> is another way to display the DataFrame by gathering the entries from all partitions into the driver.\n<br  />WARNING: The data returned to the driver must fit into the driver's available memory. If not, the driver will crash. This is only to be used if returned a small amount of data to the driver. <code>show()</code> is a better way to visualize the data and not worry about memory.</p>\n"},"dateCreated":"Oct 11, 2016 2:57:54 PM","dateStarted":"Oct 11, 2016 6:04:22 PM","dateFinished":"Oct 11, 2016 6:04:22 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6792"},{"text":"%pyspark\n\nresults = df.collect()\nprint results","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:38:15 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198031425_-334884502","id":"20161011-150031_647099533","result":{"code":"SUCCESS","type":"TEXT","msg":"[Row(name=u'Roberto', age=58, occupation=u'Professor', state=u'Nevada', credit_score=714), Row(name=u'Zoe', age=25, occupation=u'Lawyer', state=u'California', credit_score=811), Row(name=u'Robin', age=30, occupation=u'Reporter', state=u'California', credit_score=639), Row(name=u'Edward', age=72, occupation=u'Retired', state=u'Florida', credit_score=790), Row(name=u'Jack', age=16, occupation=u'Student', state=u'California', credit_score=None), Row(name=u'Zoe', age=25, occupation=u'Lawyer', state=u'California', credit_score=811), Row(name=u'Jack', age=23, occupation=u'Bartender', state=u'Nevada', credit_score=564)]\n"},"dateCreated":"Oct 11, 2016 3:00:31 PM","dateStarted":"Oct 12, 2016 6:38:15 PM","dateFinished":"Oct 12, 2016 6:38:15 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6793"},{"text":"%md\n\nHow many partitions will the DataFrame be split into? \n*Since this is a local instance of Spark there is only 1 node, therefore there will only be 1 partition*","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 6:01:30 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198169258_926089241","id":"20161011-150249_118471033","result":{"code":"SUCCESS","type":"HTML","msg":"<p>How many partitions will the DataFrame be split into?\n<br  /><em>Since this is a local instance of Spark there is only 1 node, therefore there will only be 1 partition</em></p>\n"},"dateCreated":"Oct 11, 2016 3:02:49 PM","dateStarted":"Oct 11, 2016 4:11:45 PM","dateFinished":"Oct 11, 2016 4:11:45 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6794"},{"text":"%pyspark\n\nprint df.rdd.getNumPartitions()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 4:37:34 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198193661_334760830","id":"20161011-150313_160908293","result":{"code":"SUCCESS","type":"TEXT","msg":"12\n"},"dateCreated":"Oct 11, 2016 3:03:13 PM","dateStarted":"Oct 12, 2016 4:37:34 PM","dateFinished":"Oct 12, 2016 4:37:34 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6795"},{"text":"%md\n\n### **count** total number of rows\n`count()` returns the number of rows in this DataFrame. It is also an action operation.","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 6:05:00 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198217718_1279566211","id":"20161011-150337_2134142522","result":{"code":"SUCCESS","type":"HTML","msg":"<h3><strong>count</strong> total number of rows</h3>\n<p><code>count()</code> returns the number of rows in this DataFrame. It is also an action operation.</p>\n"},"dateCreated":"Oct 11, 2016 3:03:37 PM","dateStarted":"Oct 11, 2016 6:04:57 PM","dateFinished":"Oct 11, 2016 6:04:57 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6796"},{"text":"%pyspark\n\nprint df.count()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:38:21 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198238133_-1138549978","id":"20161011-150358_1416710979","result":{"code":"SUCCESS","type":"TEXT","msg":"7\n"},"dateCreated":"Oct 11, 2016 3:03:58 PM","dateStarted":"Oct 12, 2016 6:38:21 PM","dateFinished":"Oct 12, 2016 6:38:21 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6797"},{"text":"%md\n\n### Notes on syntax\n\nWe can write any of these examples to create a new DataFrame if we wanted to keep the resulting DataFrame, which is typically what you would do in data processing. However, since these are just for learning, I'm going to just add a `show()` to the end of each. \nThis does not overwrite the DataFrame since DataFrames are immutable. \n\nFor example, to create a new DataFrame\n    filtered_df = df.filter(df.age < 32)\n    filtered_df.show()\n\nTo simply view results of a transformation\n    df.filter(df.age < 32).show()\n\n<br>\n#### Referencing Columns\nThere are a few different ways to refer to columns in PySpark\n\nUsing Strings to refer to columns\n\n    df.sort(\"age\")\n\nUsing a `column` object, there are two different notations:\n    \n    #Pandas style notation\n    df.sort(df.age)\n    \n    #subscript notation\n    df.sort(df['age'])\n    \nUsing the `col` function with the column name as a string. Must first import `col`.    \n    from pyspark.sql.functions import col\n    df.sort(col(\"age\"))\n    ","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 6:02:15 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476202731201_-1431908193","id":"20161011-161851_2137861912","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Notes on syntax</h3>\n<p>We can write any of these examples to create a new DataFrame if we wanted to keep the resulting DataFrame, which is typically what you would do in data processing. However, since these are just for learning, I'm going to just add a <code>show()</code> to the end of each.\n<br  />This does not overwrite the DataFrame since DataFrames are immutable.</p>\n<p>For example, to create a new DataFrame</p>\n<pre><code>filtered_df = df.filter(df.age &lt; 32)\nfiltered_df.show()\n</code></pre>\n<p>To simply view results of a transformation</p>\n<pre><code>df.filter(df.age &lt; 32).show()\n</code></pre>\n<p><br></p>\n<h4>Referencing Columns</h4>\n<p>There are a few different ways to refer to columns in PySpark</p>\n<p>Using Strings to refer to columns</p>\n<pre><code>df.sort(\"age\")\n</code></pre>\n<p>Using a <code>column</code> object, there are two different notations:</p>\n<pre><code>#Pandas style notation\ndf.sort(df.age)\n\n#subscript notation\ndf.sort(df['age'])\n</code></pre>\n<p>Using the <code>col</code> function with the column name as a string. Must first import <code>col</code>.</p>\n<pre><code>from pyspark.sql.functions import col\ndf.sort(col(\"age\"))\n</code></pre>\n"},"dateCreated":"Oct 11, 2016 4:18:51 PM","dateStarted":"Oct 11, 2016 6:02:12 PM","dateFinished":"Oct 11, 2016 6:02:12 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6798"},{"text":"%md\n\n###**filter**\n\n`filter()` transformation filters rows using the given condition. **where()** is an alias for **filter()**","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 6:03:59 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198291333_1897396324","id":"20161011-150451_699717170","result":{"code":"SUCCESS","type":"HTML","msg":"<h3><strong>filter</strong></h3>\n<p><code>filter()</code> transformation filters rows using the given condition. <strong>where()</strong> is an alias for <strong>filter()</strong></p>\n"},"dateCreated":"Oct 11, 2016 3:04:51 PM","dateStarted":"Oct 11, 2016 6:03:54 PM","dateFinished":"Oct 11, 2016 6:03:54 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6799"},{"text":"%md\n\nKeep only records that contain people whose age is less than 32. ","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 6:02:28 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198411231_-626035439","id":"20161011-150651_613376509","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Keep only records that contain people whose age is less than 32.</p>\n"},"dateCreated":"Oct 11, 2016 3:06:51 PM","dateStarted":"Oct 11, 2016 4:54:29 PM","dateFinished":"Oct 11, 2016 4:54:29 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6800"},{"text":"%pyspark\n\ndf.filter(df.age < 32).show()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:38:29 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198439192_257763433","id":"20161011-150719_2069768308","result":{"code":"SUCCESS","type":"TEXT","msg":"+-----+---+----------+----------+------------+\n| name|age|occupation|     state|credit_score|\n+-----+---+----------+----------+------------+\n|  Zoe| 25|    Lawyer|California|         811|\n|Robin| 30|  Reporter|California|         639|\n| Jack| 16|   Student|California|        null|\n|  Zoe| 25|    Lawyer|California|         811|\n| Jack| 23| Bartender|    Nevada|         564|\n+-----+---+----------+----------+------------+\n\n"},"dateCreated":"Oct 11, 2016 3:07:19 PM","dateStarted":"Oct 12, 2016 6:38:30 PM","dateFinished":"Oct 12, 2016 6:38:30 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6801"},{"text":"%md\n\n###**orderBy**\n`orderBy()` allows you to sort a DataFrame by one or more columns. `sort()` is an alias of `orderBy()`.","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 4:54:14 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198459565_2041505306","id":"20161011-150739_1114064979","result":{"code":"SUCCESS","type":"HTML","msg":"<h3><strong>orderBy</strong></h3>\n<p><code>orderBy()</code> allows you to sort a DataFrame by one or more columns. <code>sort()</code> is an alias of <code>orderBy()</code>.</p>\n"},"dateCreated":"Oct 11, 2016 3:07:39 PM","dateStarted":"Oct 12, 2016 4:54:11 PM","dateFinished":"Oct 12, 2016 4:54:11 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6802"},{"text":"%md\n\nLet's sort names alphabetically","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 6:02:36 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476202635446_131571774","id":"20161011-161715_1673753034","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Let's sort names alphabetically</p>\n"},"dateCreated":"Oct 11, 2016 4:17:15 PM","dateStarted":"Oct 11, 2016 4:17:27 PM","dateFinished":"Oct 11, 2016 4:17:27 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6803"},{"text":"%pyspark\n\ndf.orderBy(df.name).show()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:38:43 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198492318_1228392963","id":"20161011-150812_1915679935","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+---+----------+----------+------------+\n|   name|age|occupation|     state|credit_score|\n+-------+---+----------+----------+------------+\n| Edward| 72|   Retired|   Florida|         790|\n|   Jack| 16|   Student|California|        null|\n|   Jack| 23| Bartender|    Nevada|         564|\n|Roberto| 58| Professor|    Nevada|         714|\n|  Robin| 30|  Reporter|California|         639|\n|    Zoe| 25|    Lawyer|California|         811|\n|    Zoe| 25|    Lawyer|California|         811|\n+-------+---+----------+----------+------------+\n\n"},"dateCreated":"Oct 11, 2016 3:08:12 PM","dateStarted":"Oct 12, 2016 6:38:43 PM","dateFinished":"Oct 12, 2016 6:38:43 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6804"},{"text":"%md \n\nNow sort by oldest to youngest using the method `desc` (for sorting in descending order). The method `asc()` is the default (for sorting in ascending order)","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 4:56:54 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198533250_1537993234","id":"20161011-150853_1335400420","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now sort by oldest to youngest using the method <code>desc</code> (for sorting in descending order). The method <code>asc()</code> is the default (for sorting in ascending order)</p>\n"},"dateCreated":"Oct 11, 2016 3:08:53 PM","dateStarted":"Oct 11, 2016 4:56:50 PM","dateFinished":"Oct 11, 2016 4:56:50 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6805"},{"text":"%pyspark\n\ndf.orderBy(df.age.desc()).show()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:38:52 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198513870_371187738","id":"20161011-150833_1066833202","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+---+----------+----------+------------+\n|   name|age|occupation|     state|credit_score|\n+-------+---+----------+----------+------------+\n| Edward| 72|   Retired|   Florida|         790|\n|Roberto| 58| Professor|    Nevada|         714|\n|  Robin| 30|  Reporter|California|         639|\n|    Zoe| 25|    Lawyer|California|         811|\n|    Zoe| 25|    Lawyer|California|         811|\n|   Jack| 23| Bartender|    Nevada|         564|\n|   Jack| 16|   Student|California|        null|\n+-------+---+----------+----------+------------+\n\n"},"dateCreated":"Oct 11, 2016 3:08:33 PM","dateStarted":"Oct 12, 2016 6:38:52 PM","dateFinished":"Oct 12, 2016 6:38:52 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6806"},{"text":"%md\n\nSince ascending is the default, it will work by using either the string or column expression.\n\n    df.orderBy('age')\n    #or\n    df.orderBy(df.age)\n\nThe `desc()` method is already defined for `column` objects, however, it is not for string objects.\n\nFor example, this would not work\n    df.orderBy('age'.desc())\n\nIf you really wanted to write it that way you would have to import the `desc` method\n\n    # Import needed functions\n    from pyspark.sql.functions import desc\n    df.orderBy(\"name\", desc(\"score\"))\n    \n","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 6:02:44 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476205257318_-1215858","id":"20161011-170057_1988977078","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Since ascending is the default, it will work by using either the string or column expression.</p>\n<pre><code>df.orderBy('age')\n#or\ndf.orderBy(df.age)\n</code></pre>\n<p>The <code>desc()</code> method is already defined for <code>column</code> objects, however, it is not for string objects.</p>\n<p>For example, this would not work</p>\n<pre><code>df.orderBy('age'.desc())\n</code></pre>\n<p>If you really wanted to write it that way you would have to import the <code>desc</code> method</p>\n<pre><code># Import needed functions\nfrom pyspark.sql.functions import desc\ndf.orderBy(\"name\", desc(\"score\"))\n</code></pre>\n"},"dateCreated":"Oct 11, 2016 5:00:57 PM","dateStarted":"Oct 11, 2016 5:10:28 PM","dateFinished":"Oct 11, 2016 5:10:28 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6807"},{"text":"%md\n\n### **distinct()** and **dropDuplicates()**","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:31:30 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198574188_-1427990064","id":"20161011-150934_840939455","result":{"code":"SUCCESS","type":"HTML","msg":"<h3><strong>distinct()</strong> and <strong>dropDuplicates()</strong></h3>\n"},"dateCreated":"Oct 11, 2016 3:09:34 PM","dateStarted":"Oct 12, 2016 6:08:53 PM","dateFinished":"Oct 12, 2016 6:08:53 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6808"},{"text":"%md \n\n`distinct()` considers all columns and filters out duplicate rows to return a DataFrame with only unique rows. ","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:31:04 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198648427_-1034853309","id":"20161011-151048_1084738109","result":{"code":"SUCCESS","type":"HTML","msg":"<p><code>distinct()</code> filters out duplicate rows and considers all columns.</p>\n"},"dateCreated":"Oct 11, 2016 3:10:48 PM","dateStarted":"Oct 11, 2016 5:18:28 PM","dateFinished":"Oct 11, 2016 5:18:28 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6809"},{"text":"%pyspark\n\ndf.distinct().show()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:39:03 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198672778_1810947126","id":"20161011-151112_1795728713","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+---+----------+----------+------------+\n|   name|age|occupation|     state|credit_score|\n+-------+---+----------+----------+------------+\n|   Jack| 16|   Student|California|        null|\n|Roberto| 58| Professor|    Nevada|         714|\n|   Jack| 23| Bartender|    Nevada|         564|\n|    Zoe| 25|    Lawyer|California|         811|\n|  Robin| 30|  Reporter|California|         639|\n| Edward| 72|   Retired|   Florida|         790|\n+-------+---+----------+----------+------------+\n\n"},"dateCreated":"Oct 11, 2016 3:11:12 PM","dateStarted":"Oct 12, 2016 6:39:03 PM","dateFinished":"Oct 12, 2016 6:39:03 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6810"},{"text":"%md\n\nOne of the rows with Zoe was deleted, but the two rows with name \"Jack\" are kept because the other columns for these two rows are different and all columns in a row must match another row for it to be considered a duplicate.","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:40:42 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198704096_1068812470","id":"20161011-151144_620488542","result":{"code":"SUCCESS","type":"HTML","msg":"<p>One of the rows with Zoe was deleted, but the two rows with name &ldquo;Jack&rdquo; are kept because the other columns for these two rows are different and all columns in a row must match another row for it to be considered a duplicate.</p>\n"},"dateCreated":"Oct 11, 2016 3:11:44 PM","dateStarted":"Oct 12, 2016 6:00:37 PM","dateFinished":"Oct 12, 2016 6:00:37 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6811"},{"text":"%md\n\n`dropDuplicates()` is similar to `distinct()`, except it has the option to consider only certain columns. Let's only drop rows if they have the duplicate names.","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:09:55 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198717603_404351119","id":"20161011-151157_740369280","result":{"code":"SUCCESS","type":"HTML","msg":"<p><code>dropDuplicates()</code> is similar to <code>distinct()</code>, except it has the option to consider only certain columns. Let's only drop rows if they have the duplicate names.</p>\n"},"dateCreated":"Oct 11, 2016 3:11:57 PM","dateStarted":"Oct 11, 2016 5:18:52 PM","dateFinished":"Oct 11, 2016 5:18:52 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6812"},{"text":"%pyspark\n\ndf.dropDuplicates(['name']).show()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:39:20 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198760752_-450668206","id":"20161011-151240_1437201820","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+---+----------+----------+------------+\n|   name|age|occupation|     state|credit_score|\n+-------+---+----------+----------+------------+\n|   Jack| 23| Bartender|    Nevada|         564|\n| Edward| 72|   Retired|   Florida|         790|\n|  Robin| 30|  Reporter|California|         639|\n|Roberto| 58| Professor|    Nevada|         714|\n|    Zoe| 25|    Lawyer|California|         811|\n+-------+---+----------+----------+------------+\n\n"},"dateCreated":"Oct 11, 2016 3:12:40 PM","dateStarted":"Oct 12, 2016 6:39:20 PM","dateFinished":"Oct 12, 2016 6:39:21 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6813"},{"text":"%md\n\nNotice this drops one row for \"Zoe\" and one for \"Jack\". It kept the first row with \"Jack\" and drops the other. It's important to use a sort if you want more control over which rows are discarded. Let's sort by age so that the older Jack will be kept.","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:40:34 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198784481_-471429322","id":"20161011-151304_370020597","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Notice this drops one row for &ldquo;Zoe&rdquo; and one for &ldquo;Jack&rdquo;. It kept the first row with &ldquo;Jack&rdquo; and drops the other. It's important to use a sort if you want more control over which rows are discarded. Let's sort by age so that the older Jack will be kept.</p>\n"},"dateCreated":"Oct 11, 2016 3:13:04 PM","dateStarted":"Oct 12, 2016 6:40:33 PM","dateFinished":"Oct 12, 2016 6:40:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6814"},{"text":"%pyspark\n\ndf.sort(df.name, df.age).dropDuplicates(['name']).show()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:41:02 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476198812037_1844636463","id":"20161011-151332_1033192794","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+---+----------+----------+------------+\n|   name|age|occupation|     state|credit_score|\n+-------+---+----------+----------+------------+\n|   Jack| 16|   Student|California|        null|\n| Edward| 72|   Retired|   Florida|         790|\n|  Robin| 30|  Reporter|California|         639|\n|Roberto| 58| Professor|    Nevada|         714|\n|    Zoe| 25|    Lawyer|California|         811|\n+-------+---+----------+----------+------------+\n\n"},"dateCreated":"Oct 11, 2016 3:13:32 PM","dateStarted":"Oct 12, 2016 6:41:02 PM","dateFinished":"Oct 12, 2016 6:41:02 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6815"},{"text":"%md\n\n###**select()** and **drop()**\n\n`select()` and `drop()` are opposites of each other. We can either select specific columns we want to keep or drop a specified column from a DataFrame. Decide which would require less work, for example if you have 100 columns and you want to get rid of 5 columns then it's easier to drop those 5 than to select the 95 you want to keep.","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 2:56:03 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476199218933_225560017","id":"20161011-152018_1696243395","result":{"code":"SUCCESS","type":"HTML","msg":"<h3><strong>select()</strong> and <strong>drop()</strong></h3>\n<p><code>select()</code> and <code>drop()</code> are opposites of each other. We can either select specific columns we want to keep or drop a specified column from a DataFrame. Decide which would require less work, for example if you have 100 columns and you want to get rid of 5 columns then it's easier to drop those 5 than to select the 95 you want to keep.</p>\n"},"dateCreated":"Oct 11, 2016 3:20:18 PM","dateStarted":"Oct 11, 2016 5:22:09 PM","dateFinished":"Oct 11, 2016 5:22:09 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6816"},{"text":"%pyspark\n\ndf.select(\"name\", \"age\").show()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:41:12 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476199264596_1544371977","id":"20161011-152104_2014034991","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+---+\n|   name|age|\n+-------+---+\n|Roberto| 58|\n|    Zoe| 25|\n|  Robin| 30|\n| Edward| 72|\n|   Jack| 16|\n|    Zoe| 25|\n|   Jack| 23|\n+-------+---+\n\n"},"dateCreated":"Oct 11, 2016 3:21:04 PM","dateStarted":"Oct 12, 2016 6:41:13 PM","dateFinished":"Oct 12, 2016 6:41:13 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6817"},{"text":"%pyspark\n\ndf.drop(\"credit_score\").drop(\"occupation\").show()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:41:18 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476199291649_1289314105","id":"20161011-152131_1300446232","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+---+----------+\n|   name|age|     state|\n+-------+---+----------+\n|Roberto| 58|    Nevada|\n|    Zoe| 25|California|\n|  Robin| 30|California|\n| Edward| 72|   Florida|\n|   Jack| 16|California|\n|    Zoe| 25|California|\n|   Jack| 23|    Nevada|\n+-------+---+----------+\n\n"},"dateCreated":"Oct 11, 2016 3:21:31 PM","dateStarted":"Oct 12, 2016 6:41:18 PM","dateFinished":"Oct 12, 2016 6:41:18 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6818"},{"text":"%md\n\nNotice in `drop()` you can only have one column per `drop()`, so if you want to drop multiple columns you have to use multiple `drop()`","authenticationInfo":{},"dateUpdated":"Oct 11, 2016 6:03:11 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476199311933_-1261540454","id":"20161011-152151_634116301","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Notice in <code>drop()</code> you can only have one column per <code>drop()</code>, so if you want to drop multiple columns you have to use multiple <code>drop()</code></p>\n"},"dateCreated":"Oct 11, 2016 3:21:51 PM","dateStarted":"Oct 11, 2016 5:22:57 PM","dateFinished":"Oct 11, 2016 5:22:57 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6819"},{"text":"%md\n\n###Use **withColumn** to create a new column\n\nTo start simple, let's create a new age variable that is a person's age plus 1","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 1:44:42 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476288283180_-1538450740","id":"20161012-160443_1370795820","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Use <strong>withColumn</strong> to create a new column</h3>\n<p>To start simple, let's create a new age variable that is a person's age plus 1</p>\n"},"dateCreated":"Oct 12, 2016 4:04:43 PM","dateStarted":"Oct 13, 2016 1:44:38 PM","dateFinished":"Oct 13, 2016 1:44:38 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6820"},{"text":"%pyspark\n\ndf.withColumn(\"new_age\", df.age + 1).show()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:41:27 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476288951411_148790846","id":"20161012-161551_1182277854","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+---+----------+----------+------------+-------+\n|   name|age|occupation|     state|credit_score|new_age|\n+-------+---+----------+----------+------------+-------+\n|Roberto| 58| Professor|    Nevada|         714|     59|\n|    Zoe| 25|    Lawyer|California|         811|     26|\n|  Robin| 30|  Reporter|California|         639|     31|\n| Edward| 72|   Retired|   Florida|         790|     73|\n|   Jack| 16|   Student|California|        null|     17|\n|    Zoe| 25|    Lawyer|California|         811|     26|\n|   Jack| 23| Bartender|    Nevada|         564|     24|\n+-------+---+----------+----------+------------+-------+\n\n"},"dateCreated":"Oct 12, 2016 4:15:51 PM","dateStarted":"Oct 12, 2016 6:41:27 PM","dateFinished":"Oct 12, 2016 6:41:27 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6821"},{"text":"%md\n\nNext, let's convert the column 'name' to uppercase. In this example, rather than creating a new column we overwrite the 'name' column with the new values","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 4:28:34 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476289604899_-2064705681","id":"20161012-162644_1320221281","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Next, let's convert the column 'name' to uppercase. In this example, rather than creating a new column we overwrite the 'name' column with the new values</p>\n"},"dateCreated":"Oct 12, 2016 4:26:44 PM","dateStarted":"Oct 12, 2016 4:28:21 PM","dateFinished":"Oct 12, 2016 4:28:21 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6822"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import upper\n\ndf.withColumn(\"name\", upper(df.name)).show()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:41:35 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476289269940_559390731","id":"20161012-162109_67047306","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+---+----------+----------+------------+\n|   name|age|occupation|     state|credit_score|\n+-------+---+----------+----------+------------+\n|ROBERTO| 58| Professor|    Nevada|         714|\n|    ZOE| 25|    Lawyer|California|         811|\n|  ROBIN| 30|  Reporter|California|         639|\n| EDWARD| 72|   Retired|   Florida|         790|\n|   JACK| 16|   Student|California|        null|\n|    ZOE| 25|    Lawyer|California|         811|\n|   JACK| 23| Bartender|    Nevada|         564|\n+-------+---+----------+----------+------------+\n\n"},"dateCreated":"Oct 12, 2016 4:21:09 PM","dateStarted":"Oct 12, 2016 6:41:36 PM","dateFinished":"Oct 12, 2016 6:41:36 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6823"},{"text":"%md\n\nWe can also create a new column with conditional values. Let's specify if a person is an adult or child by their age.","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 4:45:27 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476290554136_-512878215","id":"20161012-164234_1346432555","result":{"code":"SUCCESS","type":"HTML","msg":"<p>We can also create a new column with conditional values. Let's specify if a person is an adult or child by their age.</p>\n"},"dateCreated":"Oct 12, 2016 4:42:34 PM","dateStarted":"Oct 12, 2016 4:45:25 PM","dateFinished":"Oct 12, 2016 4:45:25 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6824"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import when\n\ndf.withColumn(\"age_category\", when(df.age < 18, \"minor\").otherwise(\"adult\")).show()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:41:43 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476289720302_-1198114885","id":"20161012-162840_1690860887","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+---+----------+----------+------------+------------+\n|   name|age|occupation|     state|credit_score|age_category|\n+-------+---+----------+----------+------------+------------+\n|Roberto| 58| Professor|    Nevada|         714|       adult|\n|    Zoe| 25|    Lawyer|California|         811|       adult|\n|  Robin| 30|  Reporter|California|         639|       adult|\n| Edward| 72|   Retired|   Florida|         790|       adult|\n|   Jack| 16|   Student|California|        null|       minor|\n|    Zoe| 25|    Lawyer|California|         811|       adult|\n|   Jack| 23| Bartender|    Nevada|         564|       adult|\n+-------+---+----------+----------+------------+------------+\n\n"},"dateCreated":"Oct 12, 2016 4:28:40 PM","dateStarted":"Oct 12, 2016 6:41:43 PM","dateFinished":"Oct 12, 2016 6:41:43 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6825"},{"text":"%md\n\nAll of the examples above create a new column by using an existing column. You can add an arbitary column by using literals: `lit()`\n\nLet's add a new column called 'country' to our DataFrame and set all values to USA.","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:25:37 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476293372805_-2041859953","id":"20161012-172932_1458482282","result":{"code":"SUCCESS","type":"HTML","msg":"<p>All of the examples above create a new column by using an existing column. You can add an arbitary column by using literals: <code>lit()</code></p>\n<p>Let's add a new column called 'country' to our DataFrame and set all values to USA.</p>\n"},"dateCreated":"Oct 12, 2016 5:29:32 PM","dateStarted":"Oct 12, 2016 6:25:35 PM","dateFinished":"Oct 12, 2016 6:25:35 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6826"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import lit\n\ndf.withColumn(\"country\", lit(\"USA\")).show()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:41:52 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476293725277_-1609095706","id":"20161012-173525_483246230","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+---+----------+----------+------------+-------+\n|   name|age|occupation|     state|credit_score|country|\n+-------+---+----------+----------+------------+-------+\n|Roberto| 58| Professor|    Nevada|         714|    USA|\n|    Zoe| 25|    Lawyer|California|         811|    USA|\n|  Robin| 30|  Reporter|California|         639|    USA|\n| Edward| 72|   Retired|   Florida|         790|    USA|\n|   Jack| 16|   Student|California|        null|    USA|\n|    Zoe| 25|    Lawyer|California|         811|    USA|\n|   Jack| 23| Bartender|    Nevada|         564|    USA|\n+-------+---+----------+----------+------------+-------+\n\n"},"dateCreated":"Oct 12, 2016 5:35:25 PM","dateStarted":"Oct 12, 2016 6:41:52 PM","dateFinished":"Oct 12, 2016 6:41:52 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6827"},{"text":"%md\n\nWe can do the same if we want to add a constant column with an integer.","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 5:40:19 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476293963403_1349190970","id":"20161012-173923_471687205","result":{"code":"SUCCESS","type":"HTML","msg":"<p>We can do the same if we want to add a constant column with an integer.</p>\n"},"dateCreated":"Oct 12, 2016 5:39:23 PM","dateStarted":"Oct 12, 2016 5:40:17 PM","dateFinished":"Oct 12, 2016 5:40:17 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6828"},{"text":"%pyspark\n\ndf.withColumn(\"x\", lit(0)).show()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:41:57 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476293929963_-1657883840","id":"20161012-173849_1190630830","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+---+----------+----------+------------+---+\n|   name|age|occupation|     state|credit_score|  x|\n+-------+---+----------+----------+------------+---+\n|Roberto| 58| Professor|    Nevada|         714|  0|\n|    Zoe| 25|    Lawyer|California|         811|  0|\n|  Robin| 30|  Reporter|California|         639|  0|\n| Edward| 72|   Retired|   Florida|         790|  0|\n|   Jack| 16|   Student|California|        null|  0|\n|    Zoe| 25|    Lawyer|California|         811|  0|\n|   Jack| 23| Bartender|    Nevada|         564|  0|\n+-------+---+----------+----------+------------+---+\n\n"},"dateCreated":"Oct 12, 2016 5:38:49 PM","dateStarted":"Oct 12, 2016 6:41:57 PM","dateFinished":"Oct 12, 2016 6:41:57 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6829"},{"text":"%md\n\n### Rename columns using **withColumnRenamed**\n\n`withColumnRenamed(existing, new)` ","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 4:48:06 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476290746407_-1715558949","id":"20161012-164546_1419091049","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Rename columns using <strong>withColumnRenamed</strong></h3>\n<p><code>withColumnRenamed(existing, new)</code></p>\n"},"dateCreated":"Oct 12, 2016 4:45:46 PM","dateStarted":"Oct 12, 2016 4:47:59 PM","dateFinished":"Oct 12, 2016 4:47:59 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6830"},{"text":"%pyspark\n\ndf.withColumnRenamed(\"occupation\", \"job\").show()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:42:03 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476290782722_1059326740","id":"20161012-164622_548818808","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+---+---------+----------+------------+\n|   name|age|      job|     state|credit_score|\n+-------+---+---------+----------+------------+\n|Roberto| 58|Professor|    Nevada|         714|\n|    Zoe| 25|   Lawyer|California|         811|\n|  Robin| 30| Reporter|California|         639|\n| Edward| 72|  Retired|   Florida|         790|\n|   Jack| 16|  Student|California|        null|\n|    Zoe| 25|   Lawyer|California|         811|\n|   Jack| 23|Bartender|    Nevada|         564|\n+-------+---+---------+----------+------------+\n\n"},"dateCreated":"Oct 12, 2016 4:46:22 PM","dateStarted":"Oct 12, 2016 6:42:03 PM","dateFinished":"Oct 12, 2016 6:42:03 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6831"},{"text":"%md\n\n### **groupBy**\n\n`groupBy()` groups the DataFrame by the specified columns in order to do aggregations. Note `groupby()` is an alias for `groupBy()`. The aggregation functions typically create a new column. There are many different aggregation functions, the most common is `count()`, and a few others are `sum()`, `avg()`, `max()`, `min()`\n\nFirst, count records by 'state'","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 1:45:17 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476199341414_-1786106447","id":"20161011-152221_761401314","result":{"code":"SUCCESS","type":"HTML","msg":"<h3><strong>groupBy</strong></h3>\n<p><code>groupBy()</code> groups the DataFrame by the specified columns in order to do aggregations. Note <code>groupby()</code> is an alias for <code>groupBy()</code>. The aggregation functions typically create a new column. There are many different aggregation functions, the most common is <code>count()</code>, and a few others are <code>sum()</code>, <code>avg()</code>, <code>max()</code>, <code>min()</code></p>\n<p>First, count records by 'state'</p>\n"},"dateCreated":"Oct 11, 2016 3:22:21 PM","dateStarted":"Oct 13, 2016 1:45:14 PM","dateFinished":"Oct 13, 2016 1:45:14 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6832"},{"text":"%pyspark\n\ndf.groupBy(\"state\").count().show()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:42:28 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476199374346_-1768746774","id":"20161011-152254_1100770533","result":{"code":"SUCCESS","type":"TEXT","msg":"+----------+-----+\n|     state|count|\n+----------+-----+\n|   Florida|    1|\n|    Nevada|    2|\n|California|    4|\n+----------+-----+\n\n"},"dateCreated":"Oct 11, 2016 3:22:54 PM","dateStarted":"Oct 12, 2016 6:42:28 PM","dateFinished":"Oct 12, 2016 6:42:28 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6833"},{"text":"%md\n\nNext, let's find out the age of the youngest person in each state.","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:45:46 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476199485229_2076970080","id":"20161011-152445_132962484","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Next, let's find out the age of the youngest person in each state.</p>\n"},"dateCreated":"Oct 11, 2016 3:24:45 PM","dateStarted":"Oct 12, 2016 6:45:41 PM","dateFinished":"Oct 12, 2016 6:45:41 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6834"},{"text":"%pyspark\n\ndf.groupby(\"state\").min(\"age\").show()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 8:05:38 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476128183739_684014447","id":"20161010-193623_2107434028","result":{"code":"SUCCESS","type":"TEXT","msg":"+----------+--------+\n|     state|min(age)|\n+----------+--------+\n|   Florida|      72|\n|    Nevada|      23|\n|California|      16|\n+----------+--------+\n\n"},"dateCreated":"Oct 10, 2016 7:36:23 PM","dateStarted":"Oct 12, 2016 8:05:38 PM","dateFinished":"Oct 12, 2016 8:05:38 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6835"},{"text":"%md \n\n### Replacing null values with **fillna**\n\n`fillna(value, subset=None)`: the value parameter is the value to replace the null values with and subset is an optinal list of column names to apply this to. Alias for `na.fill()`.\n\nLet's create a DataFrame with some missing data","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 1:45:32 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476298705008_-781894162","id":"20161012-185825_1236118751","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Replacing null values with <strong>fillna</strong></h3>\n<p><code>fillna(value, subset=None)</code>: the value parameter is the value to replace the null values with and subset is an optinal list of column names to apply this to. Alias for <code>na.fill()</code>.</p>\n<p>Let's create a DataFrame with some missing data</p>\n"},"dateCreated":"Oct 12, 2016 6:58:25 PM","dateStarted":"Oct 13, 2016 1:45:29 PM","dateFinished":"Oct 13, 2016 1:45:29 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6836"},{"text":"%pyspark\n\nmissing_df = sqlContext.createDataFrame([(2, 1), (6, None), (None, 4)], (\"x\", \"y\"))\n                                 \nmissing_df.show()","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 12:33:48 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476298974817_1195328792","id":"20161012-190254_300286880","result":{"code":"SUCCESS","type":"TEXT","msg":"+----+----+\n|   x|   y|\n+----+----+\n|   2|   1|\n|   6|null|\n|null|   4|\n+----+----+\n\n"},"dateCreated":"Oct 12, 2016 7:02:54 PM","dateStarted":"Oct 13, 2016 12:33:48 PM","dateFinished":"Oct 13, 2016 12:33:48 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6837"},{"text":"%md\n\nIn the first example below, all missing values in the DataFrame are filled with zeros.\n\nIn the second example, we specify which column to fill.","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 12:14:43 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476299412455_582035756","id":"20161012-191012_323615374","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In the first example below, all missing values in the DataFrame are filled with zeros.</p>\n<p>In the second example, we specify which column to fill.</p>\n"},"dateCreated":"Oct 12, 2016 7:10:12 PM","dateStarted":"Oct 13, 2016 12:14:41 PM","dateFinished":"Oct 13, 2016 12:14:41 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6838"},{"text":"%pyspark\n\nmissing_df.fillna(0).show()","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 12:33:32 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476299327421_1202712353","id":"20161012-190847_2008604025","result":{"code":"SUCCESS","type":"TEXT","msg":"+---+---+\n|  x|  y|\n+---+---+\n|  2|  1|\n|  6|  0|\n|  0|  4|\n|  0|  9|\n+---+---+\n\n"},"dateCreated":"Oct 12, 2016 7:08:47 PM","dateStarted":"Oct 13, 2016 12:33:32 PM","dateFinished":"Oct 13, 2016 12:33:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6839"},{"text":"%pyspark\n\nmissing_df.fillna(0, \"y\").show()","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 12:33:36 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476299474425_-1741939546","id":"20161012-191114_1662739902","result":{"code":"SUCCESS","type":"TEXT","msg":"+----+---+\n|   x|  y|\n+----+---+\n|   2|  1|\n|   6|  0|\n|null|  4|\n|null|  9|\n+----+---+\n\n"},"dateCreated":"Oct 12, 2016 7:11:14 PM","dateStarted":"Oct 13, 2016 12:33:36 PM","dateFinished":"Oct 13, 2016 12:33:36 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6840"},{"text":"%md \n\n###Merging muliptle DataFrames using **join()**\n`join(other, on=None, how=None)`\n\nParameters: \n - other – Right side of the join\n - on – a string for join column name or a list of column names. If on is a string or a list of string indicating the name of the join column(s), the column(s) must exist on both sides, and this performs an equi-join.\n - how – default ‘inner’. Options: inner, outer, left_outer, right_outer, leftsemi.","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 1:45:37 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476361533612_-1889356971","id":"20161013-122533_1968784","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Merging muliptle DataFrames using <strong>join()</strong></h3>\n<p><code>join(other, on=None, how=None)</code></p>\n<p>Parameters:</p>\n<ul>\n<li>other – Right side of the join</li>\n<li>on – a string for join column name or a list of column names. If on is a string or a list of string indicating the name of the join column(s), the column(s) must exist on both sides, and this performs an equi-join.</li>\n<li>how – default ‘inner’. Options: inner, outer, left_outer, right_outer, leftsemi.</li>\n</ul>\n"},"dateCreated":"Oct 13, 2016 12:25:33 PM","dateStarted":"Oct 13, 2016 12:30:26 PM","dateFinished":"Oct 13, 2016 12:30:26 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6841"},{"text":"%md\n\nLet's create two new DataFrames to demonstrate joins.","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 1:07:35 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476362885886_1247643662","id":"20161013-124805_736344371","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Let's create two new DataFrames to demonstrate joins.</p>\n"},"dateCreated":"Oct 13, 2016 12:48:05 PM","dateStarted":"Oct 13, 2016 1:07:33 PM","dateFinished":"Oct 13, 2016 1:07:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6842"},{"text":"%pyspark \n\ndf1 = sqlContext.createDataFrame([(\"Emma\", 24, \"S\"), (\"Sophia\", 53, \"M\"), (\"Jacob\", 35, \"M\"), (\"Adam\", 38, \"S\")], (\"name\", \"age\", \"marital_stat\"))\n                                 \ndf2 = sqlContext.createDataFrame([(\"Emma\", \"Dallas\"), (\"Sophia\", \"San Francisco\"), (\"Adam\", \"Orlando\"), (\"William\", \"San Diego\")], (\"name\", \"city\"))\n\ndf1.show()\ndf2.show() ","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 1:45:54 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476361584120_361885464","id":"20161013-122624_1121349621","result":{"code":"SUCCESS","type":"TEXT","msg":"+------+---+------------+\n|  name|age|marital_stat|\n+------+---+------------+\n|  Emma| 24|           S|\n|Sophia| 53|           M|\n| Jacob| 35|           M|\n|  Adam| 38|           S|\n+------+---+------------+\n\n+-------+-------------+\n|   name|         city|\n+-------+-------------+\n|   Emma|       Dallas|\n| Sophia|San Francisco|\n|   Adam|      Orlando|\n|William|    San Diego|\n+-------+-------------+\n\n"},"dateCreated":"Oct 13, 2016 12:26:24 PM","dateStarted":"Oct 13, 2016 1:43:00 PM","dateFinished":"Oct 13, 2016 1:43:00 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6843"},{"text":"%md\n\nNow let's do an inner join on these two DataFrames by \"name\" to only keep rows that match between the column in both DataFrames.","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 12:51:26 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476362939996_1836987032","id":"20161013-124859_23140811","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now let's do an inner join on these two DataFrames by &ldquo;name&rdquo; to only keep rows that match between the column in both DataFrames.</p>\n"},"dateCreated":"Oct 13, 2016 12:48:59 PM","dateStarted":"Oct 13, 2016 12:51:24 PM","dateFinished":"Oct 13, 2016 12:51:24 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6844"},{"text":"%pyspark\n\ndf1.join(df2, \"name\", \"inner\").show()","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 1:07:53 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476363089987_1028411766","id":"20161013-125129_23710401","result":{"code":"SUCCESS","type":"TEXT","msg":"+------+---+------------+-------------+\n|  name|age|marital_stat|         city|\n+------+---+------------+-------------+\n|Sophia| 53|           M|San Francisco|\n|  Emma| 24|           S|       Dallas|\n|  Adam| 38|           S|      Orlando|\n+------+---+------------+-------------+\n\n"},"dateCreated":"Oct 13, 2016 12:51:29 PM","dateStarted":"Oct 13, 2016 1:07:53 PM","dateFinished":"Oct 13, 2016 1:07:54 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6845"},{"text":"%md\n\nSince Jacob was not in the second DataFrame and William was not in the first DataFrame, these two records are not included in the new DataFrame.\n\nLet's do an outer join to keep all records from both DataFrames.","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 1:10:45 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476364080066_1645176811","id":"20161013-130800_1545294153","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Since Jacob was not in the second DataFrame and William was not in the first DataFrame, these two records are not included in the new DataFrame.</p>\n<p>Let's do an outer join to keep all records from both DataFrames.</p>\n"},"dateCreated":"Oct 13, 2016 1:08:00 PM","dateStarted":"Oct 13, 2016 1:10:32 PM","dateFinished":"Oct 13, 2016 1:10:32 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6846"},{"text":"%pyspark\n\ndf1.join(df2, \"name\", \"outer\").show()","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 1:11:06 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476364182364_886421332","id":"20161013-130942_1544269386","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+----+------------+-------------+\n|   name| age|marital_stat|         city|\n+-------+----+------------+-------------+\n| Sophia|  53|           M|San Francisco|\n|  Jacob|  35|           M|         null|\n|   Emma|  24|           S|       Dallas|\n|William|null|        null|    San Diego|\n|   Adam|  38|           S|      Orlando|\n+-------+----+------------+-------------+\n\n"},"dateCreated":"Oct 13, 2016 1:09:42 PM","dateStarted":"Oct 13, 2016 1:11:06 PM","dateFinished":"Oct 13, 2016 1:11:07 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6847"},{"text":"%md\n\n### Using **unionAll()** to append DataFrames\n\nThe two DataFrames must contain the same number of columns and the corresponding columns must have compatible data types. Column names are only considered in the first DataFrame and ignored else where, which means it's extrememly important for the columns in both DataFrames be in the same exact order. If not, the `unionAll()` will return wrong results.\n\nLet's create a new DataFrame with names and cities, and then use `unionAll()` with the previous DataFrame we created with names and cities.","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 1:33:24 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476361468931_-1760374145","id":"20161013-122428_980310039","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Using <strong>unionAll()</strong> to append DataFrames</h3>\n<p>The two DataFrames must contain the same number of columns and the corresponding columns must have compatible data types. Column names are only considered in the first DataFrame and ignored else where, which means it's extrememly important for the columns in both DataFrames be in the same exact order. If not, the <code>unionAll()</code> will return wrong results.</p>\n<p>Let's create a new DataFrame with names and cities, and then use <code>unionAll()</code> with the previous DataFrame we created with names and cities.</p>\n"},"dateCreated":"Oct 13, 2016 12:24:28 PM","dateStarted":"Oct 13, 2016 1:33:21 PM","dateFinished":"Oct 13, 2016 1:33:21 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6848"},{"text":"%pyspark\n\n# Create a new DataFrame\ndf3 = sqlContext.createDataFrame([(\"Luke\", \"Raleigh\"), (\"Peter\", \"New York\")], (\"name\", \"city\"))\ndf3.show()\n\n# Previous DataFrame\ndf2.show()\n\ndf2.unionAll(df3).show()","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 1:34:11 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476362190721_964107793","id":"20161013-123630_1441537801","result":{"code":"SUCCESS","type":"TEXT","msg":"+-----+--------+\n| name|    city|\n+-----+--------+\n| Luke| Raleigh|\n|Peter|New York|\n+-----+--------+\n\n+-------+-------------+\n|   name|         city|\n+-------+-------------+\n|   Emma|       Dallas|\n| Sophia|San Francisco|\n|   Adam|      Orlando|\n|William|    San Diego|\n+-------+-------------+\n\n+-------+-------------+\n|   name|         city|\n+-------+-------------+\n|   Emma|       Dallas|\n| Sophia|San Francisco|\n|   Adam|      Orlando|\n|William|    San Diego|\n|   Luke|      Raleigh|\n|  Peter|     New York|\n+-------+-------------+\n\n"},"dateCreated":"Oct 13, 2016 12:36:30 PM","dateStarted":"Oct 13, 2016 1:34:12 PM","dateFinished":"Oct 13, 2016 1:34:12 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6849"},{"text":"%md\n\nWhat if in our new DataFrame the column order was different?","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 1:36:25 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476365671716_620842779","id":"20161013-133431_1802952051","result":{"code":"SUCCESS","type":"HTML","msg":"<p>What if in our new DataFrame the column order was different?</p>\n"},"dateCreated":"Oct 13, 2016 1:34:31 PM","dateStarted":"Oct 13, 2016 1:35:01 PM","dateFinished":"Oct 13, 2016 1:35:01 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6850"},{"text":"%pyspark\n\n# Reordering columns in df3\ndf3 = df3.select(\"city\", \"name\")\n\n# Check columns\nprint df3.columns\nprint df2.columns\n\ndf2.unionAll(df3).show()","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 1:40:11 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476365703931_1026414092","id":"20161013-133503_720766174","result":{"code":"SUCCESS","type":"TEXT","msg":"['city', 'name']\n['name', 'city']\n+--------+-------------+\n|    name|         city|\n+--------+-------------+\n|    Emma|       Dallas|\n|  Sophia|San Francisco|\n|    Adam|      Orlando|\n| William|    San Diego|\n| Raleigh|         Luke|\n|New York|        Peter|\n+--------+-------------+\n\n"},"dateCreated":"Oct 13, 2016 1:35:03 PM","dateStarted":"Oct 13, 2016 1:38:41 PM","dateFinished":"Oct 13, 2016 1:38:41 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6851"},{"text":"%md\n\nThe union is still performed without error because both DataFrames have two columns with the same data types, but the results are clearly wrong. As demonstrated in the code above, you can use `df.columns` to print the column names and order in a DataFrame. You can use this to ensure the columns are in the same order before doing a union. Also, as seen above, you can easily change the order by using `select()` and inputting the columns in the order you desire.","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 1:41:59 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476365770121_403028929","id":"20161013-133610_950955105","result":{"code":"SUCCESS","type":"HTML","msg":"<p>The union is still performed without error because both DataFrames have two columns with the same data types, but the results are clearly wrong. As demonstrated in the code above, you can use <code>df.columns</code> to print the column names and order in a DataFrame. You can use this to ensure the column are in the same order before doing a union. Also as seen above, you can easily change the order by using <code>select()</code> and inputting the columns in the order you desire.</p>\n"},"dateCreated":"Oct 13, 2016 1:36:10 PM","dateStarted":"Oct 13, 2016 1:41:11 PM","dateFinished":"Oct 13, 2016 1:41:11 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6852"},{"text":"%md\n\n###**Lambda Functions** and **User Defined Functions**\n\nPython supports a special syntax for building one-line anonymous functions. Lambda functions can be used wherever function objects are required. Lambda functions can take only a single expression. Using lambda functions is not ever required and you can always define a normal function instead, but in certain cases lambda functions can be more convenient and make writing code easier and cleaner. These can be used in situations where you need to create a function that is only going to be used once.\n\nIn Spark, a UDF (User Defined Function) is a special wrapper that allows you to use a feature that is not available in Spark by default. ","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 2:56:37 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476187977722_-1204300035","id":"20161011-121257_196544867","result":{"code":"SUCCESS","type":"HTML","msg":"<h3><strong>Lambda Functions</strong> and <strong>User Defined Functions</strong></h3>\n<p>Python supports a special syntax for building one-line anonymous functions. Lambda functions can be used wherever function objects are required. Lambda functions can take only a single expression. Using lambda functions is not ever required and you can always define a normal function instead, but in certain cases lambda functions can be more convenient and make writing code easier and cleaner. These can be used in situations where you need to create a function that is only going to be used once.</p>\n<p>In Spark, a UDF (User Defined Function) is a special wrapper that allows you to use a feature that is not available in Spark by default.</p>\n"},"dateCreated":"Oct 11, 2016 12:12:57 PM","dateStarted":"Oct 12, 2016 2:56:32 PM","dateFinished":"Oct 12, 2016 2:56:32 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6853"},{"text":"%md\n\nUsing out initial DataFrame, let's find the birth year of each person by substracting their age from the year 2016. There are many ways we can do this, but let's use a Lambda function and UDF to demonstate the concepts just introduced.","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 3:26:29 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476285761152_-112391763","id":"20161012-152241_580802335","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Using out initial DataFrame, let's find the birth year of each person by substracting their age from the year 2016. There are many ways we can do this, but let's use a Lambda function and UDF to demonstate the concepts just introduced.</p>\n"},"dateCreated":"Oct 12, 2016 3:22:41 PM","dateStarted":"Oct 12, 2016 3:26:26 PM","dateFinished":"Oct 12, 2016 3:26:26 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6854"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\nbirth_year = udf(lambda x: (2016 - x), StringType())\n\nlambda_df1 = df.withColumn('birth_year', birth_year(df.age))\n\nlambda_df1.show()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:45:58 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476214746470_379354100","id":"20161011-193906_130258050","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+---+----------+----------+------------+----------+\n|   name|age|occupation|     state|credit_score|birth_year|\n+-------+---+----------+----------+------------+----------+\n|Roberto| 58| Professor|    Nevada|         714|      1958|\n|    Zoe| 25|    Lawyer|California|         811|      1991|\n|  Robin| 30|  Reporter|California|         639|      1986|\n| Edward| 72|   Retired|   Florida|         790|      1944|\n|   Jack| 16|   Student|California|        null|      2000|\n|    Zoe| 25|    Lawyer|California|         811|      1991|\n|   Jack| 23| Bartender|    Nevada|         564|      1993|\n+-------+---+----------+----------+------------+----------+\n\n"},"dateCreated":"Oct 11, 2016 7:39:06 PM","dateStarted":"Oct 12, 2016 6:45:58 PM","dateFinished":"Oct 12, 2016 6:45:58 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6855"},{"text":"%md\n\nWe can also apply a UDF to a Python defined function. Using our second DataFrame with scores, let's define a function to determines a person's letter grade.","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 3:56:18 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476284290173_1442283308","id":"20161012-145810_845812561","result":{"code":"SUCCESS","type":"HTML","msg":"<p>We can also apply a UDF to a Python defined function. Let's use our second DataFrame with scores and define a function to determines a person's letter grade.</p>\n"},"dateCreated":"Oct 12, 2016 2:58:10 PM","dateStarted":"Oct 12, 2016 3:28:40 PM","dateFinished":"Oct 12, 2016 3:28:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6856"},{"text":"%pyspark\n\ndef credit_category(credit_score):\n    if credit_score > 800: return 'Excellent'\n    elif credit_score > 740: return 'Very Good'\n    elif credit_score > 670: return 'Good'\n    elif credit_score > 580: return 'Fair'\n    elif credit_score > 300: return 'Poor'\n    else: return ' '\n    \ncredit_category_udf = udf(credit_category, StringType())\n\ndf_new = df.withColumn(\"grade\", credit_category_udf(df.credit_score))\n\ndf_new.show()","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:52:09 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476286069863_-2011695700","id":"20161012-152749_1632327639","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+---+----------+----------+------------+---------+\n|   name|age|occupation|     state|credit_score|    grade|\n+-------+---+----------+----------+------------+---------+\n|Roberto| 58| Professor|    Nevada|         714|     Good|\n|    Zoe| 25|    Lawyer|California|         811|Excellent|\n|  Robin| 30|  Reporter|California|         639|     Fair|\n| Edward| 72|   Retired|   Florida|         790|Very Good|\n|   Jack| 16|   Student|California|        null|         |\n|    Zoe| 25|    Lawyer|California|         811|Excellent|\n|   Jack| 23| Bartender|    Nevada|         564|     Poor|\n+-------+---+----------+----------+------------+---------+\n\n"},"dateCreated":"Oct 12, 2016 3:27:49 PM","dateStarted":"Oct 12, 2016 6:52:09 PM","dateFinished":"Oct 12, 2016 6:52:10 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6857"},{"text":"%md\n\n###Good Programming Practices\n\nAs you began to program in Spark, the recommended way to write code is in the form:\n\n    df2 = df1.transformation1()\n    df2.action1()\n    df3 = df2.transformation2()\n    df3.action2()\n    \nCoding in this style will make debugging easier.\n\nAs you gain experience you can condense your code with the form:\n\n    df.transformation1().transformation2().action()\n    \nTry to keep your coding style readable. The above example can become messy if you add multiple operations on one line. \nYou can enclose the statement in parentheses and put each method, transformation, and action on a separate line (demonstrated below).","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 5:27:32 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476286340488_1533763770","id":"20161012-153220_1657448387","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Good Programming Practices</h3>\n<p>As you began to program in Spark, the recommended way to write code is in the form:</p>\n<pre><code>df2 = df1.transformation1()\ndf2.action1()\ndf3 = df2.transformation2()\ndf3.action2()\n</code></pre>\n<p>Coding in this style will make debugging easier.</p>\n<p>As you gain experience you can condense your code with the form:</p>\n<pre><code>df.transformation1().transformation2().action()\n</code></pre>\n<p>Try to keep your coding style readable. The above example can become messy if you add multiple operations on one line.\n<br  />You can enclose the statement in parentheses and put each method, transformation, and action on a separate line (demonstrated below).</p>\n"},"dateCreated":"Oct 12, 2016 3:32:20 PM","dateStarted":"Oct 12, 2016 5:27:28 PM","dateFinished":"Oct 12, 2016 5:27:28 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6858"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import concat, lit\n\n(df\n  .filter(df.age > 18)\n  .select(df.name, df.occupation, df.age)\n  .withColumn(\"age_range\", concat((df.age/10).cast(\"int\") * 10, lit(\"-\"), ((df.age/10).cast(\"int\") + 1) * 10))\n  .orderBy(df.name)\n  .show()\n  )","authenticationInfo":{},"dateUpdated":"Oct 12, 2016 6:52:36 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python","editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476291284760_-1913910948","id":"20161012-165444_735669125","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+----------+---+---------+\n|   name|occupation|age|age_range|\n+-------+----------+---+---------+\n| Edward|   Retired| 72|    70-80|\n|   Jack| Bartender| 23|    20-30|\n|Roberto| Professor| 58|    50-60|\n|  Robin|  Reporter| 30|    30-40|\n|    Zoe|    Lawyer| 25|    20-30|\n|    Zoe|    Lawyer| 25|    20-30|\n+-------+----------+---+---------+\n\n"},"dateCreated":"Oct 12, 2016 4:54:44 PM","dateStarted":"Oct 12, 2016 6:52:36 PM","dateFinished":"Oct 12, 2016 6:52:36 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6859"},{"text":"%md\n\n##End of tutorial","authenticationInfo":{},"dateUpdated":"Oct 13, 2016 12:24:02 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476292587830_1396910224","id":"20161012-171627_999364489","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>End of tutorial</h2>\n"},"dateCreated":"Oct 12, 2016 5:16:27 PM","dateStarted":"Oct 13, 2016 12:23:58 PM","dateFinished":"Oct 13, 2016 12:23:58 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6860"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476361193327_-254901713","id":"20161013-121953_1387405626","dateCreated":"Oct 13, 2016 12:19:53 PM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:6861"}],"name":"pyspark_tutorial","id":"2BZ8C9ASY","angularObjects":{"2BVGYPYB7":[],"2BXCMXU9P":[],"2BYJ5HCJN":[],"2BVNR16XK":[],"2BVXJASCQ":[],"2BXQDXCBZ":[],"2BVG4XQKV":[],"2BVJ4A971":[],"2BYQH8GZX":[],"2BXRGJY1N":[],"2BW4H2TA1":[],"2BZ2J4NRY":[],"2BYZNEE2Q":[],"2BWHRJRVF":[],"2BXSVW2NN":[],"2BXN4QR9T":[],"2BWEEPAE8":[],"2BYXRW8KB":[],"2BWH2PK3Z":[]},"config":{"looknfeel":"default"},"info":{}}
